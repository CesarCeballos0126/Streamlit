{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import des modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récuperer les données\n",
    "df = pd.read_csv('./Data/Donnees_Quotidiennes.csv', index_col= 0,parse_dates=['Date'],sep=';');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir le période de traitement, ici quotidien\n",
    "periods = dict([('W',52),('D',365),('S',4),('M',12)])\n",
    "PeriodName = 'D'\n",
    "PeriodValue = periods[PeriodName]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppression des champs inutiles.\n",
    "df  = df[['Code INSEE région','Région','Date','Consommation (MW)', 'Température (°C)']]\n",
    "# Supprimer les lignes avec des valeurs absents\n",
    "df.dropna(axis = 0, subset=[\"Consommation (MW)\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapter la base de données à notre frequence d'utilisation souhaitée pour les predictions\n",
    "df = pd.DataFrame(df.set_index('Date').groupby(['Code INSEE région','Région'])['Consommation (MW)','Température (°C)'].resample(PeriodName).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prévisualisation\n",
    "df = df.reset_index()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Application d'un filtre sur la région pour ne travailler pour le moment que sur la région Ile de france afin de simplifier les tests.\n",
    "df_idf = df[df['Code INSEE région']== 11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppression des devenus inutiles\n",
    "df_idf.drop(['Code INSEE région', 'Région'],inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_idf['test'] = pd.to_datetime(df_idf['Date'], format=\"%Y%m%d\", unit='D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paramétrage de la date en tant qu'index.\n",
    "df_idf.set_index('Date',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Méthode Box-Jenkins\n",
    "\n",
    "Il s'agit de notre méthode choisi pour la construction de nos modèles des séries chronologiques afin de réaliser notre projet de façon rapide, efficace et rigoureuse.\n",
    "3 étapes :\n",
    "+ Identification:\n",
    "    - Explorer les données pour en trouver une forme appropriée à la modélisation ARIMA / ARIMAX / SARIMA / SARIMAX\n",
    "        - Identifier si la série est stationnaire \n",
    "        - Trouver quelle transformation et / ou différenciation rendra la serié stationnaire - I\n",
    "        - Identifier les ordres p, q, P et Q les plus prometteurs\n",
    "        - Déterminer si la série chronologique es saisonnière, si c'est le cas, trouver sa période saisonnière\n",
    "        - Outils : traçage des séries cronologiques, Test Dicky-Fuller, pour la tranformation / differenciation (df.diff(), np.log(), np.sqrt()), l'ACF et le PACF\n",
    "\n",
    "\n",
    "+ Estimation :\n",
    "    - Utiliser les méthodes numériques pour estimer les coefficients AR / MA \n",
    "    - Ajustement du modèle (model.fit())\n",
    "    - Comparer plusieurs modeles en utilisant l'AIC et le BIC\n",
    "\n",
    "\n",
    "+ Diagnostique du modèle :\n",
    "    - Evaluer la qualité du modèle le mieux adapté :\n",
    "        - Réaliser de tests statistiques pour s'assurer que les résidus se comportent bien (les résidus ne sont pas correlées, et ils suivent une distribution normale) \n",
    "\n",
    "\n",
    "En utilisant les informations recueilles à partir de tests statistiques et et celles pendant l'étape de diagnostic, nous devons prendre une décision :\n",
    "- Le modèle est-il assez bon ou devons-nous revenir en arrièe et le retravailler ?\n",
    "- Lors de l'étape du diagnostique, si les résidus ne sont pas comme ils devraient être, nous reviendrons et repenserons nos choix dans les étapes précédentes. Et si les résidus sont corrects, nous pouvons aller de l'avant et faire des prédictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "df_idf['Consommation (MW)'].plot(ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous constatons que notre série a des modèles qui se répètent à intervalles réguliers, synonime d'une série saisonnière.\n",
    "\n",
    "Afin de continuer, nous devons séparer notre série en un jeu d'entrainement et de test.\n",
    "Ici, nous utilisons les valeurs passées pour faire des prédictions futures, et nous devrons donc diviser les données dans le temps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Séparation en un jeu d'entrainement et de test\n",
    "train=df_idf[df_idf.index < '2020-11-21']\n",
    "test=df_idf[df_idf.index > '2020-11-20']\n",
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Séparation des données meteo et conso\n",
    "train_conso = train['Consommation (MW)']\n",
    "train_meteo = train['Température (°C)']\n",
    "test_conso = test['Consommation (MW)']\n",
    "test_meteo = test['Température (°C)']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour modéliser notre série chonologique, elle doit être stationnaire, c'est-à-dire que la distribution des données ne change pas avec le temps.\n",
    "\n",
    "Pour cela, nous devons vérifier que :\n",
    "+ la série n'a aucune tendance\n",
    "+ la variance est constante (la distance moyenne des points de données de la ligne 0 ne change pas)\n",
    "+ L'autocorrélation est constante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function qui permet de tester si les données sont stationnaires.\n",
    "\n",
    "def get_stationarity(timeseries):\n",
    "    \n",
    "    # Statistiques mobiles\n",
    "    rolling_mean = timeseries.rolling(window=PeriodValue).mean()\n",
    "    rolling_std = timeseries.rolling(window=PeriodValue).std()\n",
    "    \n",
    "    # tracé statistiques mobiles\n",
    "    original = plt.plot(timeseries, color='blue', label='Origine')\n",
    "    mean = plt.plot(rolling_mean, color='red', label='Moyenne Mobile')\n",
    "    std = plt.plot(rolling_std, color='black', label='Ecart-type Mobile')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('Moyenne et écart-type Mobiles')\n",
    "    plt.show(block=False)\n",
    "    \n",
    "    # Test Dickey–Fuller :\n",
    "    result = adfuller(timeseries, regression='ct')\n",
    "    print('Statistiques ADF : {}'.format(result[0]))\n",
    "    print('p-value : {}'.format(result[1]))\n",
    "    print('Valeurs Critiques :')\n",
    "    for key, value in result[4].items():\n",
    "        print('\\t{}: {}'.format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tester la série temporelle\n",
    "get_stationarity(train_conso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La moyenne mobile a une legere tendance à la baisse, il existe alors une tendance negative.\n",
    "\n",
    "Le Test Dickey–Fuller nous donne un p-value inférieur à 0.05, on rejet H0 et on retient alors l'hypothese de stationarité. \n",
    "Neamoins, dû à la présence de tendance, nous devons alors proceder à la differenciation de la série"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effectuer la differenciation de la série, puis supprimer les Nan\n",
    "train_stationary = train_conso.diff().dropna()\n",
    "\n",
    "# Tester la stationnarité de notre nouvelle série\n",
    "get_stationarity(train_stationary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* H0:la série a une racine unitaire, ce qui indiquerait qu'elle est non stationnaire.\n",
    "* H1: H0 est rejetée et la série n'a pas de structure temporellement dépendante : elle est stationnaire.\n",
    "\n",
    "Nous utiliserons la p-valeur de ce test pour déterminer l'hypothèse à retenir au seuil de 5%.\n",
    "* p valeu > 0.05 : H0 n'est pas rejetée. La série n'est pas stationnaire\n",
    "* p valeu ≤ 0.05 : H0 est rejetée et la série est stationnaire.\n",
    "\n",
    "Avec une valeur critique de 5% et un p-value de 5.08e-10, (p valeu ≤ 0.05) nous rejettons notre Hyphothese nulle, donc notre série est stationnaire.\n",
    "Nous constantons aussi que la moyenne Mobile n'a plus de tendance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, nous utiliserons la méthode seasonal_decompose afin d'analyser la composante saisonnière, confirmer l'absence de tendance et le résiduel.\n",
    "\n",
    "Alors, pour décomposer les données, nous devons savoir à quelle fréquence les cycles se répètent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trouver les périods / fréquence\n",
    "from statsmodels.graphics.tsaplots import plot_pacf, plot_acf\n",
    "\n",
    "fig, ax = plt.subplots(figsize= (17,5))\n",
    "plot_acf(train_conso, lags = 730, ax=ax, zero=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons clairement voir qu'il y a une période saisonnière de 365 étapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, nous devons trouver l'ordre de différenciation.\n",
    "Pour rendre une série chronologique stationnaire, nous pouvons avoir besoin d'appliquer une différenciation saisonnière (au lieu de soustraire le plus récent valeur de la série chronologique, nous soustrayons la valeur de série chronologique d'un cycle antérieur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# le parametre Period est à modifier selon la frequence determinée par l'ACF\n",
    "# [('W',52),('D',365),('S',4),('M',12)]\n",
    "# On applique la méthode seasonal_decompose à notre série d'entraînement\n",
    "res = sm.tsa.seasonal_decompose(train_conso,period=365)\n",
    "fig = res.plot()\n",
    "fig.set_figheight(5)\n",
    "fig.set_figwidth(10)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette décomposition nous permet d'identifier une tendance descroissante linéairement, ainsi qu'une saisonnalité de période 12 (annuelle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On applique la méthode seasonal_decompose à notre série d'entraînement différenciée\n",
    "res = sm.tsa.seasonal_decompose(train_stationary,period=365)\n",
    "fig = res.plot()\n",
    "fig.set_figheight(5)\n",
    "fig.set_figwidth(10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On observe une tendance plutot constant, mais des résidus avec trop de variations en fonction du temps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous constatons un cycle saisonnier, alors nous prendrons la différence saisonnière.\n",
    "\n",
    "à retenir : Jamais faire plus de deux ordres de différenciation au total\n",
    "\n",
    "Une fois que nous avons trouvé les deux ordres de différenciation, et fait les séries chronologiques stationnaires, nous devons trouver les autres paramètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Différence \n",
    "# diff(1), différenciation \"d\"\n",
    "# diff(365), longueur du cycle saisonnier, différenciation saisonniière \"D\"\n",
    "train_stationary_s = train_conso.diff().diff(365).dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifier les ordres p,q non saisonnières et les ordres P,Q saisonnières les plus prometteurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracer la fonction d'autocorrélation simple et la fonction d'autocorrélation partielle \n",
    "# de la série différenciée\n",
    "from statsmodels.graphics.tsaplots import plot_pacf, plot_acf\n",
    "\n",
    "plt.figure(figsize= (17,5))\n",
    "plt.subplot(121)\n",
    "plot_acf(train_stationary, lags = 50, zero = False, ax=plt.gca())\n",
    "plt.subplot(122)\n",
    "plot_pacf(train_stationary, lags = 50, zero = False, ax=plt.gca())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La zone bleutée représente la zone de non significativité des autocorrélogrammes. \n",
    "Les graphiques de l'ACF et de la PACF indiquent un \"p\" = (0) et un \"q\" = (0)  \n",
    "\n",
    "Pour rappel, on se base ici sur le fait que dans un modèle  AR(p) l'autocorrélogramme simple s'annule au rang  p+1 et pour un modèle  MA(q) l'autocorrélogramme partiel s'annule au rang  q+1 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracer la fonction d'autocorrélation simple et la fonction d'autocorrélation partielle \n",
    "# de la série différenciée s\n",
    "\n",
    "# Liste de lags, corresponde à la frequence avec laquelle un evenetment se repete, chaque 365 etapes\n",
    "lags = [1,365,730]\n",
    "\n",
    "plt.figure(figsize= (17,5))\n",
    "plt.subplot(121)\n",
    "plot_acf(train_stationary_s, lags = lags, zero = False, ax=plt.gca())\n",
    "plt.subplot(122)\n",
    "plot_pacf(train_stationary_s, lags = lags, zero = False, ax=plt.gca())\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La zone bleutée représente la zone de non significativité des autocorrélogrammes. \n",
    "Les graphiques de l'ACF et de la PACF indiquent un \"P\" = (1) et un \"Q\" aussi = (0)  \n",
    "\n",
    "\n",
    "*On va donc fitter un modèle SARIMA(0,1,0)(1,1,0,7) + la variable externe : Temperature*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import seaborn as sns\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set(font='IPAGothic')\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as stats\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarimax_model = SARIMAX(train_conso, order=(0,1,0), seasonal_order=(1,1,0,7), exog=train_meteo,\n",
    "                enforce_stationarity=False, enforce_invertibility=False, freq='D', trend='n')\n",
    "res = sarimax_model.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons utiliser l'outil d'evaluation \"Critére d'Information Akaike - AIC\", afin d'évaluer la bonne adéquation du modèle pour les predictions et comparer aussi plusieurs modèles.\n",
    "\n",
    "Rappel :\n",
    "* Un modèle qui fait des meilleurs prévisions reçoit un score AIC inférieur\n",
    "* AIC privilege les modèles simples, donc moins de parametres\n",
    "\n",
    "L'outil \"BIC - Critère d'Information Bayésien\" est similaire à l'AIC, mais il est utilisé pour trouver le meilleur modèle explicative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afin de vérifier que les paramtres choisis rendrent le meilleur modèle, nous allons tester d'autres parametres afin de trouver le meilleur score AIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pmdarima as pm\n",
    "\n",
    "results = pm.auto_arima(train_conso,\n",
    "                        d= 1,                        # Ordre de différenciation non saisonnière\n",
    "                        max_p = 3,\n",
    "                        max_q = 3,\n",
    "                        seasonal = True,             # Car notre serie est bien saisonnière\n",
    "                        stationary=False,            # Notre sèrie n'est pas stationnaire\n",
    "                        m= 7,                        # périod saisonnière\n",
    "                        D = 1,                       # Ordre de différenciation saisonnière\n",
    "                        max_P = 3,\n",
    "                        max_Q = 3,\n",
    "                        information_criterion='aic', # Afin de choisir le meilleur modèle basé sur l'AIC\n",
    "                        trace = True,                # afficher l'AIC du modèle\n",
    "                        error_action = 'ignore',     # ignorer les ordres qui ne marchent pas\n",
    "                        stepwise = True,             # méthode de recherche intelligente\n",
    "                        trend=None,                  # Tendance\n",
    "                        scoring='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La méthode auto_arima nous indique que le meilleur modèle serait un SARIMAX (3,1,0)(3,1,0)[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher les résultats du modèle SARIMAX\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnostique du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pour diagnostiquer notre modèle, nous allons analyser les résidus des données d'entraînement.\n",
    "- Les résidus sont la différence entre nos modèles de prediction et les valeurs réelles de la série\n",
    "- Pour un modèle idéal, les résidus doivent être du bruit blanc (chaque valeur n'est pas corrélée avec les valeurs précédentes), non corrélé centré sur 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyser les résidus du model avec plot_diagnostics\n",
    "sns.set_style(\"white\")\n",
    "results.plot_diagnostics()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'histogramme nous montre la distribution mesurée, ligne verte montre une distribution normale et la ligne orange montre une version lisée de cet histogramme. \n",
    "Si notre modèle est bon, ces deux lignes devraient être presque identiques. Ce n'est pas exactement le cas pour notre modèle.\n",
    "\n",
    "La graphique Q-Q Normal, est une autre façon de montrer la distribution des résidus du modèle et vérifier s'ils suivent une distribution normale. Pour cela, les points blues doivent se trouver au tour de la ligne rouge. Ce n'est pas exactement le cas pour notre modèle.\n",
    "\n",
    "Le correlogramme est une trace  ACF des résidus, 95% des corrélations pour un décalage supérieur à zéro de devraient pas être significatives. S'il y a une corrélation significative dans les résidus, cela signifie qu'il y a des informations dans les données que notre modèle n'a pas saisies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prob(Q) est la valeur de p associée à l'hypothèse nulle selon laquelle les résidus n'ont pas de structure de corrélation\n",
    "- Prob(JB) est la valeur de p associée à l'hypothèse nulle selon laquelle les résidus suivent une distribution normale\n",
    "\n",
    "Si une des ces valeurs est inférieur à 0.05 nous rejetons l'hypothèse\n",
    "* p valeu > 0.05 : H0 n'est pas rejetée. \n",
    "* p valeu ≤ 0.05 : H0 est rejetée\n",
    "\n",
    "Ici, avec Prob(Q) = 0.47 nous ne rejetons pas H0, cela signifie que nos résidus n'ont pas de structure de corrélation \n",
    "Par contre, nous rejetons H0 pour Prob(JB), donc les résidus ne suivent pas une distribution normale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il semble que nous devons faire quelques ajustements sur les parametres de différenciation afin de trouver un bon modèle ou trouver un autre moyen pour transformer la série"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Avec un modèle SARIMAX (3,1,0)(3,1,0)[7]\n",
    "model = SARIMAX(train_conso, order=(3,1,0), seasonal_order=(3,1,0,7), exog=train_meteo, \n",
    "                enforce_stationarity=False, enforce_invertibility=False, freq='D', trend='n').fit()\n",
    "\n",
    "# \"Steps\" concerne le nombre de predictions à efectuer\n",
    "forecast = model.get_forecast(steps=10, exog = test_meteo)\n",
    "\n",
    "# Determiner nos intervalles de confiance\n",
    "mean_forecast = forecast.predicted_mean\n",
    "confidence_intervals = forecast.conf_int()\n",
    "lower_limits = confidence_intervals['lower Consommation (MW)']\n",
    "upper_limits = confidence_intervals['upper Consommation (MW)']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrice = pd.DataFrame({'Reel': test['Consommation (MW)'], \n",
    "            'predits' : mean_forecast.values,\n",
    "            'lower_limits' : lower_limits,\n",
    "            'upper_limits' : upper_limits,\n",
    "            'T' : test['Température (°C)'] }, index = test.index)\n",
    "matrice.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graphique\n",
    "font1 = {'family':'Times','color':'darkblue','size':18}\n",
    "font2 = {'family':'Times','color':'black','size':14}\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(test_conso.index, test_conso, label='Valeurs réellees', color='blue', marker='o',linestyle='-.')\n",
    "plt.plot(mean_forecast.index, mean_forecast.values, label=\"Valeurs predites\", color='red', marker='o',linestyle='-.')\n",
    "plt.fill_between(mean_forecast.index, lower_limits, upper_limits, color='pink')\n",
    "plt.legend(labelcolor='markerfacecolor')\n",
    "plt.title(\"Prédiction avec le modèle SARIMAX (3,1,0)(3,1,0)[7] Quotidien\", fontdict = font1)\n",
    "plt.xlabel('Date', fontdict = font2)\n",
    "plt.ylabel('Moyenne de Consommation (MW)', fontdict = font2)\n",
    "plt.xticks(rotation=45)\n",
    "#plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import pi\n",
    "from bokeh.plotting import figure, output_notebook, show, ColumnDataSource\n",
    "from bokeh.models import Legend, DatetimeTickFormatter, formatters, HoverTool, LinearAxis, Range1d\n",
    "\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source\n",
    "source = ColumnDataSource(matrice)\n",
    "\n",
    "# List de tools\n",
    "TOOLS=\"crosshair,pan,wheel_zoom,box_zoom,reset\"\n",
    "\n",
    "p = figure(plot_width = 600, plot_height = 400,     \n",
    "           title = \"Prédiction avec le modèle SARIMAX (3,1,0)(3,1,0)[7] Quotidien\",                    \n",
    "           x_axis_label = 'Date', x_axis_type=\"datetime\", y_axis_label ='Moyenne de Consommation (MW)',\n",
    "           tools=TOOLS)\n",
    "\n",
    "\n",
    "p.title.text_color = \"darkblue\"\n",
    "p.title.text_font = \"times\"\n",
    "p.title.text_font_size = \"20px\"\n",
    "p.title.align = 'center'\n",
    "\n",
    "\n",
    "p.varea(x = 'Date', y1 = 'lower_limits', y2='upper_limits', color='pink', alpha=0.5, source = source)\n",
    "\n",
    "p.line(x = 'Date', y='Reel', color = \"navy\", legend_label = \"Valeurs réellees\", source = source)   \n",
    "p.circle(x = 'Date', y='Reel', color = \"navy\",fill_color='white', size=8, source = source)\n",
    "\n",
    "p.line(x = 'Date', y='predits', color = \"red\", legend_label = \"Valeurs predites\", source = source) \n",
    "p.circle(x = 'Date', y='predits', color = \"red\", fill_color='white',size=8, source = source)\n",
    "\n",
    "p.xaxis.major_label_orientation = pi/4\n",
    "p.xgrid.grid_line_color = None\n",
    "p.ygrid.grid_line_color = None\n",
    "p.xaxis.ticker.desired_num_ticks = 10\n",
    "\n",
    "\n",
    "# Activation de l'interaction avec la légende\n",
    "p.legend.location = \"top_left\"\n",
    "p.legend.click_policy = 'hide'\n",
    "\n",
    "# Style hover\n",
    "p.add_tools(HoverTool(\n",
    "    tooltips=[('Date', '@Date{%Y-%m-%d}'),\n",
    "        ('Prédiction', '@predits{0.00}'),\n",
    "        ('Valeur réelle', '@Reel{0.00}')],formatters={'@Date': 'datetime'}))\n",
    "\n",
    "show(p);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le nivel de prediction n'est pas loin des valeurs réelles, neamoins nous avons constaté que les résidus n'avaient pas une distribution normale, c'est à dire que les résidus n'arrivaient pas à expliquer les variations du modèle, ce qui nous pouvons constater sur le graphique. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source\n",
    "source = ColumnDataSource(matrice)\n",
    "\n",
    "\n",
    "# List de tools\n",
    "TOOLS=\"crosshair,pan,wheel_zoom,box_zoom,reset\"\n",
    "\n",
    "\n",
    "y_overlimit = 0.05 \n",
    "p = figure(plot_width = 600, plot_height = 400,     \n",
    "           title = \"Prédiction avec le modèle Elastic Net Quotidien\",                    \n",
    "           x_axis_label = 'Date', x_axis_type=\"datetime\",\n",
    "           y_axis_label = 'Consommation Moyenne',\n",
    "           toolbar_location=\"below\",\n",
    "           tools=TOOLS)  \n",
    "\n",
    "\n",
    "p.title.text_color = \"darkblue\"\n",
    "p.title.text_font = \"times\"\n",
    "p.title.text_font_size = \"20px\"\n",
    "p.title.align = 'center'\n",
    "\n",
    "p.varea(x = 'Date', y1 = 'lower_limits', y2='upper_limits', color='pink', alpha=0.5, source = source)\n",
    "\n",
    "p.line(x='Date', y = 'Reel', color = \"navy\", legend_label = \"Valeurs réellees\", source = source)   \n",
    "p.circle(x='Date', y ='Reel', color = \"navy\",fill_color='white', size=8, source = source)\n",
    "\n",
    "p.line(x='Date', y ='predits', color = \"red\", legend_label = \"Valeurs predites\", source = source) \n",
    "p.circle(x='Date', y='predits', color = \"red\", fill_color='white',size=8, source = source)\n",
    "\n",
    "# axis y, gauche\n",
    "p.y_range = Range1d(matrice['Reel'].min() * (1 - y_overlimit), matrice['Reel'].max() * (1 + y_overlimit))\n",
    "\n",
    "p.xaxis.major_label_orientation = pi/4\n",
    "p.xgrid.grid_line_color = None\n",
    "p.ygrid.grid_line_color = None\n",
    "p.xaxis.ticker.desired_num_ticks = 10\n",
    "\n",
    "# Axis y, droite\n",
    "y_column2_range = \"T\" + \"_range\"\n",
    "p.extra_y_ranges = {\n",
    "    y_column2_range: Range1d(\n",
    "        start=matrice['T'].min() * (1 - y_overlimit),\n",
    "        end=matrice['T'].max() * (1 + y_overlimit),\n",
    "    )\n",
    "}\n",
    "p.add_layout(LinearAxis(y_range_name=y_column2_range), \"right\")\n",
    "\n",
    "p.line( x='Date', y = 'T', color=\"grey\", legend_label=\"T (C°)\", y_range_name=y_column2_range, source = source)\n",
    "p.circle(x='Date', y = 'T', color = \"grey\",fill_color='white', size=8, y_range_name=y_column2_range, source = source)\n",
    "\n",
    "\n",
    "# Activation de l'interaction avec la légende\n",
    "p.legend.location = \"top_center\"\n",
    "p.legend.click_policy = 'hide'\n",
    "\n",
    "# Style hover\n",
    "p.add_tools(HoverTool(\n",
    "    tooltips=[('Date', '@Date{%Y-%m-%d}'),\n",
    "        ('Prédiction', '@predits{0.00}'),\n",
    "        ('Valeur réelle', '@Reel{0.00}'),\n",
    "        ('C°', \"@T{0.00}\")],\n",
    "    formatters={'@Date': 'datetime'}\n",
    "))\n",
    "\n",
    "show(p);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer et afficher le score MAPE\n",
    "y_true, y_pred = np.array(test['Consommation (MW)']), np.array(mean_forecast)\n",
    "MAPE = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "print(\"Mean Absolute Prediction Error : %0.2f%%\"% MAPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec notre Modèle ARIMA(3,1,0)(3,1,0)[7] nous avons une probabilité moyenne d'erreur de **4.41%**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Racine carrée de la Moyenne des résidus au carré.\n",
    "# RMSE\n",
    "\n",
    "def rmse(predictions, targets):\n",
    "    return np.sqrt(((predictions - targets) ** 2).mean())\n",
    "\n",
    "rmse(mean_forecast, test['Consommation (MW)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plus l'erreur quadratique moyenne est proche de 0, plus précises sont les prédictions.\n",
    "\n",
    "Nous pourrons donc comparer ce résultat avec le RMSE des autres modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MSE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mean_squared_error(test,y_pred)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0adcc2737ebf6a4a119f135174df96668767fca1ef1112612db5ecadf2b6d608"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "metadata": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}